{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing libraries:\n",
    "\n",
    "1. Tensorflow\n",
    "2. Keras\n",
    "3. matplotlib\n",
    "4. numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U tensorflow keras matplotlib numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disabling the warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## Loading the dataset\n",
    "\n",
    "the Fashion MNIST counts with 70000 of images splided in 10 types of clothes and accessories:\n",
    "\n",
    "0. T-shirt/top\n",
    "1. Trouser\n",
    "2. Pullover\n",
    "3. Dress\n",
    "4. Coat\n",
    "5. Sandal\n",
    "6. Shirt\n",
    "7. Sneaker\n",
    "8. Bag\n",
    "9. Ankle boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (60000, 28, 28)\n",
      "Test shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "# Get the dataset\n",
    "((X_train_ori, y_train_ori), (X_test_ori, y_test_ori)) = fashion_mnist.load_data()\n",
    "\n",
    "# Create a reference for the labels\n",
    "labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Print a summary aboout the train dataset\n",
    "print(\"Train shape:\", X_train_ori.shape)\n",
    "print(\"Test shape:\", X_test_ori.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x700 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure the size of the image\n",
    "plt.figure(figsize=(15,7))\n",
    "\n",
    "# Plot the images\n",
    "for i in range(10):\n",
    "  plt.subplot(2, 5, i+1)\n",
    "  plt.imshow(X_train_ori[i])\n",
    "  plt.title(labels[y_train_ori[i]])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the dataset\n",
    "\n",
    "- Reshape the image vector from 2D images to 3D with this distribuition (inputs size, height, columns, colors). That reshape basicly add one more dimesion corresponding the color, just is a pattern,.\n",
    "- Rescale the value of each one pixel. THe variace between 0 and 255 probably will incresse too much the value of the weight, making the train more difficult. To solve that problem we rescale into 0 and 1.\n",
    "- Tranform the result to categorial. The Fashion MNIST split the classes of the clothes using values between 0 and 9, but isn't scalar problem, so it's necessary tranform into a vector with mutiples results.\n",
    "E.g.: Case the value is `4` the result will be `[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "# Reshape to 4D array (inputs size, height, columns, colors)\n",
    "X_train = X_train_ori.reshape(X_train_ori.shape[0], 28, 28, 1)\n",
    "# Normalize the value from 0 to 1.0\n",
    "X_train = X_train.astype('float32')\n",
    "X_train/=255\n",
    "\n",
    "# Reshape to 4D array (inputs size, height, columns, colors)\n",
    "X_test = X_test_ori.reshape(X_test_ori.shape[0], 28, 28, 1)\n",
    "# Normalize the value from 0 to 1.0\n",
    "X_test = X_test.astype('float32')\n",
    "X_test/=255\n",
    "\n",
    "# Transforma in categorical results\n",
    "y_train = np_utils.to_categorical(y_train_ori)\n",
    "y_test= np_utils.to_categorical(y_test_ori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 500\n",
    "OUTPUT_SIZE = len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0924 16:54:20.897780 20012 deprecation_wrapper.py:119] From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0924 16:54:20.914704 20012 deprecation_wrapper.py:119] From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0924 16:54:20.917695 20012 deprecation_wrapper.py:119] From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0924 16:54:20.947614 20012 deprecation_wrapper.py:119] From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0924 16:54:20.948110 20012 deprecation_wrapper.py:119] From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0924 16:54:21.140633 20012 deprecation_wrapper.py:119] From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0924 16:54:21.148614 20012 deprecation.py:506] From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0924 16:54:21.497676 20012 deprecation_wrapper.py:119] From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Dense, Conv2D, MaxPool2D, Flatten, BatchNormalization\n",
    "\n",
    "# Create a network\n",
    "model = Sequential()\n",
    "\n",
    "# ====================\n",
    "#     First Layer\n",
    "model.add(Conv2D(32, (3,3), padding='same', input_shape=(28, 28, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "\n",
    "model.add(Conv2D(32, (3,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "\n",
    "model.add(MaxPool2D(2, 2))\n",
    "model.add(Dropout(0.2))\n",
    "# ====================\n",
    "\n",
    "# ====================\n",
    "#    Second Layer\n",
    "model.add(Conv2D(64, (3,3), padding='same', input_shape=(28, 28, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "\n",
    "model.add(MaxPool2D(2, 2))\n",
    "model.add(Dropout(0.3))\n",
    "# ====================\n",
    "\n",
    "# ====================\n",
    "#      Flatten\n",
    "model.add(Flatten())\n",
    "# ===================\n",
    "\n",
    "# ====================\n",
    "#  First Dense Layer\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "# ====================\n",
    "\n",
    "# ====================\n",
    "#  Final Dense Layer\n",
    "model.add(Dense(OUTPUT_SIZE))\n",
    "model.add(Activation('softmax'))\n",
    "# ====================\n",
    "\n",
    "# Define the optmizer and loss function\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Result of traing')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.plot(history.history['acc'], label=\"Training Accuracy\")\n",
    "plt.plot(history.history['val_acc'], label=\"Validation Accuracy\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
